{
 "cells": [
  {
   "cell_type": "code",
   "id": "53392fb1-e34c-4921-8272-778e69266aa9",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_path = \"./results/checkpoint-472\"\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"medicalai/ClinicalBERT\")\n",
    "#model = AutoModelForQuestionAnswering.from_pretrained(\"medicalai/ClinicalBERT\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"medicalai/ClinicalBERT\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "\n",
    "# Example/dummy dataset: each sample contains a context, a question, and the ground-truth answer.\n",
    "dataset = load_dataset('json', data_files='dataset.json')['train']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T20:28:25.191395Z",
     "start_time": "2025-04-13T20:28:24.278159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Util functions\n",
    "\n",
    "def flatten_squad_like_data(example):\n",
    "    new_contexts = []\n",
    "    new_questions = []\n",
    "    new_answers = []\n",
    "\n",
    "    # Pull out the context just once\n",
    "    context_text = example[\"context\"]\n",
    "\n",
    "    for i in range(0,  len(context_text)):\n",
    "        # Each 'qa' within 'qas' is a question + list of answers\n",
    "        for qa in example[\"qas\"][i]:\n",
    "            answers = qa[\"answers\"]\n",
    "            for answer in answers:\n",
    "                new_contexts.append(context_text[i])\n",
    "                new_questions.append(qa[\"question\"])\n",
    "                new_answers.append({\"text\": answer[\"text\"], \"answer_start\": answer[\"answer_start\"]})\n",
    "\n",
    "    # Return the new, flattened lists. We rely on the 'map' call with 'batched=True'\n",
    "    # to automatically collate these into a single output set of columns.\n",
    "    return {\n",
    "        \"context\": new_contexts,\n",
    "        \"question\": new_questions,\n",
    "        \"answers\": new_answers\n",
    "    }\n",
    "\n",
    "# Calculates the most similar questions and returns the contexts from the dataset\n",
    "import evaluate\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load MedicalBERT tokenizer and model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "def get_contexts(user_question):\n",
    "    # Calculate the embedding for the user question\n",
    "    user_embedding = get_embedding(user_question)\n",
    "\n",
    "    # Compute cosine similarity with each question in the dataset and store results\n",
    "    results = []\n",
    "    for entry in dataset:\n",
    "        context = entry[\"context\"]\n",
    "        for qa in entry[\"qas\"]:\n",
    "            question = qa[\"question\"]\n",
    "            question_embedding = get_embedding(question)\n",
    "            # Compute cosine similarity between the user question embedding and the dataset question embedding\n",
    "            similarity = F.cosine_similarity(user_embedding, question_embedding, dim=0)\n",
    "            results.append({\n",
    "                \"context\": context,\n",
    "                \"question\": question,\n",
    "                \"similarity\": similarity.item()\n",
    "            })\n",
    "\n",
    "    # Get the top 3 similar questions (highest cosine similarity)\n",
    "    top_results = sorted(results, key=lambda x: x[\"similarity\"], reverse=True)[:3]\n",
    "\n",
    "    top_contexts_dict = {entry[\"context\"]: entry for entry in top_results}\n",
    "    unique_results = list(top_contexts_dict.values())\n",
    "\n",
    "    result_to_return = \"\"\n",
    "    # Display the top matching contexts, questions, and similarity scores\n",
    "    for result in unique_results:\n",
    "        result_to_return += result[\"context\"]\n",
    "\n",
    "    return result_to_return\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"\n",
    "    Calculates the embedding for a given text using the MedicalBERT model.\n",
    "\n",
    "    This function tokenizes the input text, runs the model to extract the hidden states,\n",
    "    and then performs mean pooling over the last hidden state to generate a fixed-size embedding.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Enable output_hidden_states so we can use the hidden representations\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    # Use the last hidden state [batch, sequence_length, hidden_dim]\n",
    "    # and then perform mean pooling over the sequence_length dimension.\n",
    "    last_hidden_state = outputs.hidden_states[-1]\n",
    "    embedding = last_hidden_state.mean(dim=1)  # shape: [batch, hidden_dim]\n",
    "\n",
    "    return embedding.squeeze(0)  # Remove batch dimension for similarity computation\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    For each example, tokenize the question and context together. Also, convert the raw answer span (start position in chars)\n",
    "    into token indices that the model will use as labels.\n",
    "    \"\"\"\n",
    "    # Tokenize question and context together.\n",
    "    inputs = tokenizer(examples[\"question\"], examples[\"context\"],\n",
    "                       truncation=True,\n",
    "                       padding=\"max_length\",\n",
    "                       max_length=256,\n",
    "                       return_offsets_mapping=True)\n",
    "\n",
    "    offset_mappings = inputs.pop(\"offset_mapping\")  # remove offsets (used only to align char indices)\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    #cada elemento no \"offsets\" representa a posiçao inicial e posiçao final de cada token da concatenação da questão com o contexto\n",
    "    for i, offsets in enumerate(offset_mappings):\n",
    "        # For the first answer in each example\n",
    "        answer = examples[\"answers\"][i][\"text\"]\n",
    "        answer_start_char = examples[\"answers\"][i][\"answer_start\"]\n",
    "        answer_end_char = answer_start_char + len(answer)\n",
    "\n",
    "        # Find the token indices corresponding to the start and end character positions.\n",
    "        start_index = None\n",
    "        end_index = None\n",
    "        for idx, (start_char, end_char) in enumerate(offsets):\n",
    "            if start_char <= answer_start_char < end_char:\n",
    "                start_index = idx\n",
    "            if start_char < answer_end_char <= end_char:\n",
    "                end_index = idx\n",
    "        # In some cases the answer might not align perfectly with a token span.\n",
    "        if start_index is None:\n",
    "            start_index = 0\n",
    "        if end_index is None:\n",
    "            end_index = len(offsets) - 1\n",
    "\n",
    "        start_positions.append(start_index)\n",
    "        end_positions.append(end_index)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "\n",
    "def postprocess_qa_predictions(predictions, features, max_answer_length=30):\n",
    "    \"\"\"\n",
    "    Convert raw model predictions (start/end logits) into answer text predictions.\n",
    "    Assumes that the examples and features have an 'id' field to uniquely identify each example.\n",
    "    This example is simplified; see Hugging Face's run_qa.py for a complete version.\n",
    "    \"\"\"\n",
    "    all_start_logits, all_end_logits = predictions\n",
    "    predictions_dict = {}\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        # We assume each feature has an \"offset_mapping\" that maps tokens to character spans,\n",
    "        # and an \"example_id\" to link back to the original example.\n",
    "        offsets = feature[\"offset_mapping\"]\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        start_logits = all_start_logits[i]\n",
    "        end_logits = all_end_logits[i]\n",
    "\n",
    "        # Find the best token span for the answer\n",
    "        best_score = -float(\"inf\")\n",
    "        best_start, best_end = None, None\n",
    "        for start_index, start_logit in enumerate(start_logits):\n",
    "            for end_index, end_logit in enumerate(end_logits[start_index:]):\n",
    "                # Check condition: maximum answer length, valid offsets (ignore special tokens)\n",
    "                if (end_index + start_index) < len(offsets) and (end_index <= max_answer_length):\n",
    "                    score = start_logit + end_logit\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_start = start_index\n",
    "                        best_end = start_index + end_index\n",
    "        # Convert token indices to answer text using offset mappings\n",
    "        if best_start is not None and best_end is not None:\n",
    "            start_char = offsets[best_start][0]\n",
    "            end_char = offsets[best_end][1]\n",
    "            # Use the original context to recover text; assuming feature has a \"context\" key.\n",
    "            answer_text = feature[\"context\"][start_char: end_char]\n",
    "        else:\n",
    "            answer_text = \"\"\n",
    "        # Map predictions to example IDs (this requires that your features have an \"id\" or \"example_id\")\n",
    "        example_id = feature[\"id\"] if \"id\" in feature else feature.get(\"example_id\", i)\n",
    "        predictions_dict[example_id] = answer_text\n",
    "\n",
    "    # Build a list of prediction dicts as expected by the squad_metric.\n",
    "    predictions_list = [{\"id\": id_, \"prediction_text\": text} for id_, text in predictions_dict.items()]\n",
    "    return predictions_list\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    raw_predictions, raw_label_ids = eval_pred\n",
    "\n",
    "    # Postprocess predictions to generate answer texts (implementation omitted for brevity)\n",
    "    predictions = postprocess_qa_predictions(raw_predictions, tokenized_val_dataset)\n",
    "\n",
    "    # Create references by enumerating through the validation dataset\n",
    "    references = []\n",
    "    for idx, example in enumerate(tokenized_val_dataset):\n",
    "        # Use the index as a string id\n",
    "        references.append({\n",
    "            \"id\": str(idx),\n",
    "            \"answers\": example[\"answers\"]  # assuming this is formatted as needed\n",
    "        })\n",
    "\n",
    "    results = squad_metric.compute(predictions=predictions, references=references)\n",
    "    return {\n",
    "        \"exact_match\": results[\"exact_match\"],\n",
    "        \"f1\": results[\"f1\"]\n",
    "    }\n",
    "\n",
    "def add_dummy_ids(example, idx):\n",
    "    example[\"id\"] = str(idx)\n",
    "    return example\n"
   ],
   "id": "b8df4aae53c1f7c9",
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "id": "ca9012e87ecc0204",
   "metadata": {},
   "source": [
    "# Fine tune the model\n",
    "\n",
    "train_dataset = dataset.map(\n",
    "    flatten_squad_like_data,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Tokenize the dataset.\n",
    "tokenized_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "tokenized_train_dataset = split_dataset[\"train\"]\n",
    "tokenized_val_dataset = split_dataset[\"test\"]\n",
    "tokenized_val_dataset = tokenized_val_dataset.map(add_dummy_ids, with_indices=True)\n",
    "# Set up training arguments.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results2\",\n",
    "    #evaluation_strategy=\"no\",  # for simplicity we won't run an evaluation loop here\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    save_steps=1000  # Save infrequently for the demo\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with our model, training args, and tokenized dataset.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Fine tune the model on the QA dataset.\n",
    "trainer.train()\n",
    "\n",
    "# -------------------------\n",
    "# Testing/inference step:\n",
    "# Let’s take a question and its context, then have the model extract an answer.\n",
    "\n",
    "question = \"What medication was given?\"\n",
    "context = \"In the hospital, the patient was administered amoxicillin for a bacterial infection.\"\n",
    "\n",
    "# Tokenize the input: note that for QA we combine question and context.\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# The model outputs 'start_logits' and 'end_logits', which represent the probability distribution\n",
    "# for the start and end of the answer span.\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "# Identify the most likely token positions for the start and end of the answer.\n",
    "start_index = torch.argmax(start_logits, dim=1).item()\n",
    "end_index = torch.argmax(end_logits, dim=1).item()\n",
    "\n",
    "# Decode the tokens corresponding to the predicted span.\n",
    "input_ids = inputs[\"input_ids\"].squeeze()\n",
    "answer_tokens = input_ids[start_index:(end_index + 1)]\n",
    "answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Output the results.\n",
    "print(\"Question:\", question)\n",
    "print(\"Predicted Answer:\", answer)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T20:29:16.205462Z",
     "start_time": "2025-04-13T20:29:12.653832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model metrics\n",
    "predictions, label_ids, metrics = trainer.predict(tokenized_val_dataset)\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Metrics:\", eval_results)"
   ],
   "id": "40b5e9b9dd9cf432",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'offset_mapping'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[53], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Model metrics\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m predictions, label_ids, metrics \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenized_val_dataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m eval_results \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mevaluate()\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvaluation Metrics:\u001B[39m\u001B[38;5;124m\"\u001B[39m, eval_results)\n",
      "File \u001B[1;32mD:\\LNSCIA-desafio\\.venv\\lib\\site-packages\\transformers\\trainer.py:4232\u001B[0m, in \u001B[0;36mTrainer.predict\u001B[1;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[0;32m   4229\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m   4231\u001B[0m eval_loop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprediction_loop \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39muse_legacy_prediction_loop \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluation_loop\n\u001B[1;32m-> 4232\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43meval_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   4233\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdescription\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mPrediction\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric_key_prefix\u001B[49m\n\u001B[0;32m   4234\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4235\u001B[0m total_batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39meval_batch_size \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mworld_size\n\u001B[0;32m   4236\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetric_key_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_jit_compilation_time\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m output\u001B[38;5;241m.\u001B[39mmetrics:\n",
      "File \u001B[1;32mD:\\LNSCIA-desafio\\.venv\\lib\\site-packages\\transformers\\trainer.py:4443\u001B[0m, in \u001B[0;36mTrainer.evaluation_loop\u001B[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[0;32m   4441\u001B[0m     eval_set_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlosses\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m all_losses \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m args\u001B[38;5;241m.\u001B[39minclude_for_metrics \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   4442\u001B[0m     eval_set_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minputs\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m all_inputs \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minputs\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m args\u001B[38;5;241m.\u001B[39minclude_for_metrics \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 4443\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   4444\u001B[0m \u001B[43m        \u001B[49m\u001B[43mEvalPrediction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredictions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mall_preds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mall_labels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43meval_set_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4445\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4446\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m metrics \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   4447\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m {}\n",
      "Cell \u001B[1;32mIn[48], line 184\u001B[0m, in \u001B[0;36mcompute_metrics\u001B[1;34m(eval_pred)\u001B[0m\n\u001B[0;32m    181\u001B[0m raw_predictions, raw_label_ids \u001B[38;5;241m=\u001B[39m eval_pred\n\u001B[0;32m    183\u001B[0m \u001B[38;5;66;03m# Postprocess predictions to generate answer texts (implementation omitted for brevity)\u001B[39;00m\n\u001B[1;32m--> 184\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43mpostprocess_qa_predictions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_predictions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenized_val_dataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;66;03m# Create references by enumerating through the validation dataset\u001B[39;00m\n\u001B[0;32m    187\u001B[0m references \u001B[38;5;241m=\u001B[39m []\n",
      "Cell \u001B[1;32mIn[51], line 146\u001B[0m, in \u001B[0;36mpostprocess_qa_predictions\u001B[1;34m(predictions, features, max_answer_length)\u001B[0m\n\u001B[0;32m    141\u001B[0m predictions_dict \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, feature \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(features):\n\u001B[0;32m    144\u001B[0m     \u001B[38;5;66;03m# We assume each feature has an \"offset_mapping\" that maps tokens to character spans,\u001B[39;00m\n\u001B[0;32m    145\u001B[0m     \u001B[38;5;66;03m# and an \"example_id\" to link back to the original example.\u001B[39;00m\n\u001B[1;32m--> 146\u001B[0m     offsets \u001B[38;5;241m=\u001B[39m \u001B[43mfeature\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moffset_mapping\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m    147\u001B[0m     input_ids \u001B[38;5;241m=\u001B[39m feature[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    148\u001B[0m     start_logits \u001B[38;5;241m=\u001B[39m all_start_logits[i]\n",
      "\u001B[1;31mKeyError\u001B[0m: 'offset_mapping'"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Test pose question1\n",
    "\n",
    "question1 = \"What does Alzheimer's disease cause?\"\n",
    "context1 = get_contexts(question1)\n",
    "\n",
    "# Tokenize the input: note that for QA we combine question and context.\n",
    "inputs = tokenizer(question1, context1, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# The model outputs 'start_logits' and 'end_logits', which represent the probability distribution\n",
    "# for the start and end of the answer span.\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "# Identify the most likely token positions for the start and end of the answer.\n",
    "start_index = torch.argmax(start_logits, dim=1).item()\n",
    "end_index = torch.argmax(end_logits, dim=1).item()\n",
    "\n",
    "# Decode the tokens corresponding to the predicted span.\n",
    "input_ids = inputs[\"input_ids\"].squeeze()\n",
    "answer_tokens = input_ids[start_index:(end_index + 1)]\n",
    "answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Output the results.\n",
    "print(\"Question:\", question1)\n",
    "print(\"Predicted Answer:\", answer)"
   ],
   "id": "1bab308e7f4baa37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Test pose question2 with pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Sample context and question (replace these wi\n",
    "#\n",
    "# th your own data)\n",
    "question2 = \"O que acontece socialmente com pessoas com Alzheimer em grande parte dos casos?\"\n",
    "context2 = get_contexts(question2)\n",
    "\n",
    "print(\"Context:\", context2)\n",
    "\n",
    "# Get the model's answer\n",
    "result_pipeline = qa_pipeline(context=context2, question=question2)\n",
    "\n",
    "print(\"Answer:\", result_pipeline[\"answer\"])"
   ],
   "id": "d2baa4be2ad95bae",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
