{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T18:52:41.097601Z",
     "start_time": "2025-04-14T18:52:28.560392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "model_folder = \"./results2/checkpoint-3880\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"medicalai/ClinicalBERT\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_folder)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# -------------------\n",
    "# 2. Load the Validation Dataset\n",
    "# -------------------\n",
    "# Assuming the dataset is in a SQuAD-like JSON format.\n",
    "dataset = load_dataset('json', data_files='dataset_augmented.json')['train']\n",
    "# Optionally, split off a portion for evaluation.\n",
    "split_dataset = dataset.train_test_split(test_size=0.1)\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# 3. Preprocess the Data\n",
    "# -------------------\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    For each example, tokenize the question and context together. Also, convert the raw answer span (start position in chars)\n",
    "    into token indices that the model will use as labels.\n",
    "    \"\"\"\n",
    "    # Tokenize question and context together.\n",
    "    inputs = tokenizer(examples[\"question\"], examples[\"context\"],\n",
    "                       truncation=True,\n",
    "                       padding=\"max_length\",\n",
    "                       max_length=256,\n",
    "                       return_offsets_mapping=True)\n",
    "\n",
    "    offset_mappings = inputs.pop(\"offset_mapping\")  # remove offsets (used only to align char indices)\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    #cada elemento no \"offsets\" representa a posiçao inicial e posiçao final de cada token da concatenação da questão com o contexto\n",
    "    for i, offsets in enumerate(offset_mappings):\n",
    "        # For the first answer in each example\n",
    "        answer = examples[\"answers\"][i][\"text\"]\n",
    "        answer_start_char = examples[\"answers\"][i][\"answer_start\"]\n",
    "        answer_end_char = answer_start_char + len(answer)\n",
    "\n",
    "        # Find the token indices corresponding to the start and end character positions.\n",
    "        start_index = None\n",
    "        end_index = None\n",
    "        for idx, (start_char, end_char) in enumerate(offsets):\n",
    "            if start_char <= answer_start_char < end_char:\n",
    "                start_index = idx\n",
    "            if start_char < answer_end_char <= end_char:\n",
    "                end_index = idx\n",
    "        # In some cases the answer might not align perfectly with a token span.\n",
    "        if start_index is None:\n",
    "            start_index = 0\n",
    "        if end_index is None:\n",
    "            end_index = len(offsets) - 1\n",
    "\n",
    "        start_positions.append(start_index)\n",
    "        end_positions.append(end_index)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "def flatten_squad_like_data(example):\n",
    "    new_contexts = []\n",
    "    new_questions = []\n",
    "    new_answers = []\n",
    "\n",
    "    # Pull out the context just once\n",
    "    context_text = example[\"context\"]\n",
    "\n",
    "    for i in range(0,  len(context_text)):\n",
    "        # Each 'qa' within 'qas' is a question + list of answers\n",
    "        for qa in example[\"qas\"][i]:\n",
    "            answers = qa[\"answers\"]\n",
    "            for answer in answers:\n",
    "                new_contexts.append(context_text[i])\n",
    "                new_questions.append(qa[\"question\"])\n",
    "                new_answers.append({\"text\": answer[\"text\"], \"answer_start\": answer[\"answer_start\"]})\n",
    "\n",
    "    # Return the new, flattened lists. We rely on the 'map' call with 'batched=True'\n",
    "    # to automatically collate these into a single output set of columns.\n",
    "    return {\n",
    "        \"context\": new_contexts,\n",
    "        \"question\": new_questions,\n",
    "        \"answers\": new_answers\n",
    "    }\n",
    "\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    flatten_squad_like_data,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Tokenize the validation dataset.\n",
    "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=val_dataset.column_names)\n",
    "\n",
    "\n",
    "# Optionally, add dummy ids (or ensure your dataset has a unique id per sample).\n",
    "def add_dummy_ids(example, idx):\n",
    "    example[\"id\"] = str(idx)\n",
    "    return example\n",
    "\n",
    "\n",
    "tokenized_val_dataset = tokenized_val_dataset.map(add_dummy_ids, with_indices=True)\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# 4. Define Inference Functions\n",
    "# -------------------\n",
    "def predict(example):\n",
    "    \"\"\"\n",
    "    Given a single example, perform the tokenization,\n",
    "    run the model to get logits, and decode the predicted answer span.\n",
    "    \"\"\"\n",
    "    # Tokenize the input (question and context) just as during preprocessing.\n",
    "    inputs = tokenizer(example[\"question\"], example[\"context\"],\n",
    "                       return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get logits for start and end positions.\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "\n",
    "    # Choose the token with maximum score for start and end.\n",
    "    start_index = torch.argmax(start_logits, dim=1).item()\n",
    "    end_index = torch.argmax(end_logits, dim=1).item()\n",
    "\n",
    "    # Decode the answer span\n",
    "    input_ids = inputs[\"input_ids\"].squeeze()\n",
    "    answer_tokens = input_ids[start_index:end_index + 1]\n",
    "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "\n",
    "def generate_predictions(tokenized_dataset):\n",
    "    \"\"\"\n",
    "    Iterate through the tokenized dataset and generate predictions.\n",
    "    Builds a predictions list and a corresponding list of reference annotations.\n",
    "    \"\"\"\n",
    "    predictions_list = []\n",
    "    references_list = []\n",
    "\n",
    "    for index, example in enumerate(tokenized_dataset):\n",
    "        pred_answer = predict(example)\n",
    "\n",
    "        # Save prediction in the SQuAD expected format.\n",
    "        predictions_list.append({\"id\": str(index), \"prediction_text\": pred_answer})\n",
    "        # Reference should be formatted with an \"answers\" key.\n",
    "        references_list.append({\n",
    "            \"id\": str(index),\n",
    "            \"answers\": [example[\"answers\"]]\n",
    "        })\n",
    "\n",
    "    return predictions_list, references_list\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# 5. Compute the Metrics (F1 and Exact Match)\n",
    "# -------------------\n",
    "# Load the squad evaluation metric.\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "\n",
    "# Generate predictions on the validation set.\n",
    "predictions, references = generate_predictions(val_dataset)\n",
    "\n",
    "# Compute metrics.\n",
    "results = squad_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(\"Eval dataset size\", len(val_dataset))\n",
    "print(\"F1 Score:\", results[\"f1\"])\n",
    "print(\"Exact Match:\", results[\"exact_match\"])"
   ],
   "id": "7082ed5c0afd7948",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/134 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93a76be31e9344e890c670fa792ecbbf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/216 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "072438a3c7354270b8e62cc446947616"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/216 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75992add020244adae68ed8dd73e4fbd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Eval dataset size 216\n",
      "F1 Score: 54.5583708773515\n",
      "Exact Match: 30.09259259259259\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
